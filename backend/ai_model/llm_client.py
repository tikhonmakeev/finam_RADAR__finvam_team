"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π LLM –∫–ª–∏–µ–Ω—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Ollama, OpenAI –∏ OpenRouter
"""
import os
import logging
import json
import aiohttp
import asyncio
from typing import Optional, Dict, Any, Union, List

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç—ã
# try:
#     from .openai_client import OpenAIClient
# except ImportError:
#     OpenAIClient = None

from .openrouter_client import OpenRouterClient

class LLMClient:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π LLM –∫–ª–∏–µ–Ω—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º."""
        self.provider = os.getenv('LLM_PROVIDER', 'ollama').lower()
        self.timeout = 30  # –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        self.client = None
        self._model_checked = False
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        if self.provider == 'openrouter':
            try:
                self.client = OpenRouterClient()
                logger.info(f"üåê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenRouter —Å –º–æ–¥–µ–ª—å—é: {os.getenv('OPENROUTER_MODEL')}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenRouter: {e}")
                raise
        # elif self.provider == 'openaiapi' and OpenAIClient:
        #     self.client = OpenAIClient()
        #     logger.info(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI API —Å –º–æ–¥–µ–ª—å—é: {self.client.model}")
        else:  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º Ollama
            self.base_url = os.getenv("OLLAMA_API_URL", "http://localhost:11434/api")
            self.model = os.getenv("OLLAMA_MODEL", "phi4-mini:latest")
            self.session = None
            logger.info(f"üîÆ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Ollama —Å –º–æ–¥–µ–ª—å—é: {self.model}")
            # –ù–µ –∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            # –û–Ω–∞ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ

    async def _check_model_availability(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."""
        if self._model_checked:
            return
            
        try:
            session = await self._get_session()
            async with session.get(f"{self.base_url}/tags") as resp:
                if resp.status == 200:
                    models = await resp.json()
                    available_models = [m['name'] for m in models.get('models', [])]
                    logger.info(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")
                    if self.model not in available_models:
                        logger.warning(f"–ú–æ–¥–µ–ª—å {self.model} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")
            self._model_checked = True
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {e}")
            # –ü—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞ –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—Ä–æ—Å–µ
            self._model_checked = False

    async def _get_session(self) -> aiohttp.ClientSession:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ aiohttp."""
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession()
        return self.session

    async def __aenter__(self):
        await self._get_session()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if hasattr(self, 'session') and self.session:
            await self.session.close()

    async def _make_request(self, endpoint: str, data: Dict) -> Optional[Dict]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ API Ollama."""
        if not hasattr(self, 'session'):
            logger.error("–°–µ—Å—Å–∏—è –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä (async with).")
            return None
            
        session = await self._get_session()
        url = f"{self.base_url}/{endpoint.lstrip('/')}"
        
        try:
            async with session.post(
                url,
                json=data,
                timeout=aiohttp.ClientTimeout(total=self.timeout)
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    logger.error(f"–û—à–∏–±–∫–∞ API ({response.status}): {error_text}")
                    return None
                return await response.json()
                
        except asyncio.TimeoutError:
            logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ {url} (—Ç–∞–π–º–∞—É—Ç: {self.timeout}—Å)")
            return None
        except aiohttp.ClientError as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å API: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ API: {str(e)}", exc_info=True)
            return None

    async def generate_response(
        self, 
        prompt: str, 
        system_prompt: str = None, 
        **kwargs
    ) -> Optional[Union[str, Dict]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ LLM.

        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            system_prompt: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                - temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-1.0)
                - max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                - top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top-p

        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–Ω–µ—à–Ω–∏–π –∫–ª–∏–µ–Ω—Ç (OpenRouter, OpenAI API)
        if self.client and hasattr(self.client, 'generate_response'):
            return await self.client.generate_response(prompt, system_prompt, **kwargs)
            
        # –î–ª—è Ollama –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ
        if not self._model_checked:
            await self._check_model_availability()
            
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama
        return await self._ollama_generate(prompt, system_prompt, **kwargs)
        
    async def _ollama_generate(
        self, 
        prompt: str, 
        system_prompt: str = None, 
        **kwargs
    ) -> Optional[Union[str, Dict]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama API.
        """
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        data = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": kwargs.get('temperature', 0.1),
                "top_p": kwargs.get('top_p', 0.9),
                "max_tokens": kwargs.get('max_tokens', 1000)
            }
        }

        result = await self._make_request("chat", data)
        if not result or 'message' not in result or 'content' not in result['message']:
            return None
            
        return result['message']['content'].strip()

    async def chat_completion(self, messages: list, **kwargs) -> Optional[str]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π chat completion –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "user", "content": "..."}, ...]
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        if not messages:
            logger.error("–°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
            return None
            
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–Ω–µ—à–Ω–∏–π –∫–ª–∏–µ–Ω—Ç (OpenRouter, OpenAI API)
        if self.client and hasattr(self.client, 'generate_response'):
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞
            prompt = ""
            system_prompt = None
            
            for msg in messages:
                if msg.get('role') == 'system':
                    system_prompt = msg.get('content', '')
                elif msg.get('role') == 'user':
                    prompt = msg.get('content', '')
            
            return await self.client.generate_response(prompt, system_prompt, **kwargs)
            
        # –î–ª—è Ollama –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
        formatted_messages = []
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            formatted_messages.append(f"{role}: {content}")
            
        return await self._ollama_generate("\n".join(formatted_messages), **kwargs)
    
    async def get_provider_info(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ
        """
        info = {
            'provider': self.provider,
            'status': 'active' if hasattr(self, 'client') and self.client else 'inactive'
        }
        
        if self.provider == 'openrouter' and hasattr(self, 'client') and self.client:
            info['model'] = self.client.model
        # elif self.provider == 'openaiapi' and hasattr(self, 'client') and self.client:
        #     info['model'] = self.client.model
        elif self.provider == 'ollama':
            info['model'] = getattr(self, 'model', 'unknown')
            
        return info

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
llm_client = LLMClient()