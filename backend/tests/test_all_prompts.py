"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ prompts.
–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤.
"""
import os
import sys
import logging
import requests
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dotenv import load_dotenv

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.append(project_root)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ü–æ–ª—É—á–∞–µ–º URL –∏–∑ .env –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
LLM_API_URL = os.getenv('LLM_API_URL', 'http://127.0.0.1:11434')

# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
TEST_CASES = {
    'tagger': [
        ("–ö–æ–º–ø–∞–Ω–∏—è –ì–∞–∑–ø—Ä–æ–º —É–≤–µ–ª–∏—á–∏–ª–∞ –¥–æ–±—ã—á—É –≥–∞–∑–∞ –Ω–∞ 10%", "–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑"),
        ("Apple –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –Ω–æ–≤—ã–π iPhone —Å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º", "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"),
        ("–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –ø–æ–≤—ã—Å–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É –¥–æ 15%", "–§–∏–Ω–∞–Ω—Å—ã"),
        ("–í –ú–æ—Å–∫–≤–µ –æ—Ç–∫—Ä—ã–ª—Å—è –Ω–æ–≤—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä", "–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–π —Å–µ–∫—Ç–æ—Ä"),
        ("–ù–æ—Ä–∏–ª—å—Å–∫–∏–π –Ω–∏–∫–µ–ª—å —Å–æ–æ–±—â–∏–ª –æ —Ä–æ—Å—Ç–µ –¥–æ–±—ã—á–∏ –ø–∞–ª–ª–∞–¥–∏—è", "–ú–µ—Ç–∞–ª–ª—ã –∏ –¥–æ–±—ã—á–∞"),
        ("–°–±–µ—Ä–±–∞–Ω–∫ –∑–∞–ø—É—Å—Ç–∏–ª —Å–µ—Ä–≤–∏—Å —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –ø–ª–∞—Ç–µ–∂–µ–π", "–§–∏–Ω—Ç–µ—Ö"),
    ],
    'comparison': [
        (("–ì–∞–∑–ø—Ä–æ–º —Å–æ–æ–±—â–∏–ª –æ —Ä–æ—Å—Ç–µ –≤—ã—Ä—É—á–∫–∏ –Ω–∞ 12% –≤ 2024 –≥–æ–¥—É.",
          "–ì–∞–∑–ø—Ä–æ–º –æ—Ç—á–∏—Ç–∞–ª—Å—è: –≤—ã—Ä—É—á–∫–∞ –≤—ã—Ä–æ—Å–ª–∞ –Ω–∞ 12%, –ø—Ä–∏–±—ã–ª—å —Å–Ω–∏–∑–∏–ª–∞—Å—å –Ω–∞ 5%."), "True"),
        (("–¶–ë –†–æ—Å—Å–∏–∏ –æ–±—ä—è–≤–∏–ª –æ —Å–Ω–∏–∂–µ–Ω–∏–∏ –∫–ª—é—á–µ–≤–æ–π —Å—Ç–∞–≤–∫–∏.",
          "–ú–∏–Ω—Ñ–∏–Ω –ø—Ä–µ–¥–ª–æ–∂–∏–ª –ø–æ–≤—ã—Å–∏—Ç—å –Ω–∞–ª–æ–≥ –Ω–∞ –ø—Ä–∏–±—ã–ª—å –∫–æ–º–ø–∞–Ω–∏–π."), "False"),
    ],
    'market_impact': [
        ("–ö–æ–º–ø–∞–Ω–∏—è X –æ–±—ä—è–≤–∏–ª–∞ –æ —Ä–µ–∫–æ—Ä–¥–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏ –≤ 15 –º–ª–Ω –¥–æ–ª–ª–∞—Ä–æ–≤ –≤ —ç—Ç–æ–º –∫–≤–∞—Ä—Ç–∞–ª–µ. –ê–∫—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 15% –ø–æ—Å–ª–µ –≤—ã—Ö–æ–¥–∞ –æ—Ç—á–µ—Ç–∞.",
         {"impact_level": "high", "affected_sectors": ["X"]}),
    ],
    'style_news': [
        ("–∫–æ–º–ø–∞–Ω–∏—è –≥–∞–∑–ø—Ä–æ–º —É–≤–µ–ª–∏—á–∏–ª–∞ –¥–æ–±—ã—á—É –≥–∞–∑–∞ –Ω–∞ 10% –≤ —ç—Ç–æ–º –≥–æ–¥—É",
         "–ö–æ–º–ø–∞–Ω–∏—è –ì–∞–∑–ø—Ä–æ–º —É–≤–µ–ª–∏—á–∏–ª–∞ –¥–æ–±—ã—á—É –≥–∞–∑–∞ –Ω–∞ 10% –≤ —ç—Ç–æ–º –≥–æ–¥—É."),
    ],
    'update_summary': [
        (("–†–∞–Ω–µ–µ —Å–æ–æ–±—â–∞–ª–æ—Å—å –æ —Ä–æ—Å—Ç–µ –≤—ã—Ä—É—á–∫–∏ –Ω–∞ 10%.",
          "–ü–æ —É—Ç–æ—á–Ω–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º, –≤—ã—Ä—É—á–∫–∞ –≤—ã—Ä–æ—Å–ª–∞ –Ω–∞ 12%."),
         "–ü–æ —É—Ç–æ—á–Ω–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º, –≤—ã—Ä—É—á–∫–∞ –≤—ã—Ä–æ—Å–ª–∞ –Ω–∞ 12%."),
    ]
}

def get_fast_model() -> str:
    """–í—ã–±–∏—Ä–∞–µ–º –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö"""
    try:
        response = requests.get(f"{LLM_API_URL}/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [m['name'] for m in models]
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –±—ã—Å—Ç—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π
            fast_models = [
                'phi4-mini:latest',
                'llama3.2:1b',
                'llama3.2:3b',
                'llama2',
                'mistral'
            ]

            # –ò—â–µ–º –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å –∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
            for fast_model in fast_models:
                for available_model in model_names:
                    if fast_model in available_model.lower():
                        logger.info(f"üöÄ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {available_model}")
                        return available_model

            # –ï—Å–ª–∏ –±—ã—Å—Ç—Ä—ã—Ö –Ω–µ—Ç, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
            if model_names:
                logger.info(f"‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å: {model_names[0]}")
                return model_names[0]
            
            logger.error("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
            return ""
        
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {response.status_code}")
        return ""
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –º–æ–¥–µ–ª–∏: {e}")
        return ""

async def test_prompt(prompt_type: str, system_prompt: str, test_cases: List[Tuple[Any, Any]], model_name: str):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
    logger.info(f"\n{'='*80}")
    logger.info(f"–¢–ï–°–¢–ò–†–£–ï–ú –ü–†–û–ú–ü–¢: {prompt_type.upper()}")
    logger.info("-" * 80)
    
    success_count = 0
    
    for i, (test_input, expected_output) in enumerate(test_cases, 1):
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é user_prompt
            user_prompt = ""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            if isinstance(test_input, tuple):
                # –î–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                if prompt_type == 'comparison':
                    user_prompt = (
                        f"–ù–æ–≤–æ—Å—Ç—å A: \"{test_input[0]}\"\n"
                        f"–ù–æ–≤–æ—Å—Ç—å B: \"{test_input[1]}\"\n\n"
                        "–í—ã—Ö–æ–¥:\n"
                        '"""'
                    )
                elif prompt_type == 'update_summary':
                    user_prompt = (
                        f"–°—É—â–µ—Å—Ç–≤—É—é—â–∞—è —Å–≤–æ–¥–∫–∞: \"{test_input[0]}\"\n\n"
                        f"–ù–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: \"{test_input[1]}\"\n\n"
                        "–û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å–≤–æ–¥–∫–∞:"
                    )
            
            # –ï—Å–ª–∏ user_prompt –Ω–µ –±—ã–ª —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º test_input –∫–∞–∫ –µ—Å—Ç—å
            if not user_prompt:
                user_prompt = str(test_input)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏
            response = requests.post(
                f"{LLM_API_URL}/api/generate",
                json={
                    "model": model_name,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.0,
                        "num_predict": 100,
                    }
                },
                timeout=300
            )
            
            if response.status_code != 200:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ API ({response.status_code}) –¥–ª—è —Ç–µ—Å—Ç–∞ {i}")
                continue
                
            result = response.json()
            response_text = result.get('response', '').strip()
            
            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            cleaned_response = response_text.split('\n')[0].strip()
            cleaned_response = cleaned_response.replace('"', '').replace("'", "")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if isinstance(expected_output, dict):
                # –î–ª—è —Å–ª–æ–≤–∞—Ä–µ–π –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
                try:
                    result_dict = eval(cleaned_response)
                    is_match = all(k in result_dict and result_dict[k] == v 
                                 for k, v in expected_output.items())
                    result_str = str(result_dict)
                except:
                    is_match = False
                    result_str = cleaned_response
            else:
                is_match = expected_output.lower() in cleaned_response.lower()
                result_str = cleaned_response
            
            if is_match:
                success_count += 1
                logger.info(f"‚úÖ –¢–µ—Å—Ç {i}: –û–ö")
            else:
                logger.warning(f"‚ö†Ô∏è –¢–µ—Å—Ç {i}: –û–®–ò–ë–ö–ê")
                logger.info(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: {expected_output}")
                logger.info(f"   –ü–æ–ª—É—á–µ–Ω–æ: {result_str}")
            
            logger.info(f"   –û—Ç–≤–µ—Ç: {result_str[:200]}..." if len(result_str) > 200 else f"   –û—Ç–≤–µ—Ç: {result_str}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ç–µ—Å—Ç–∞ {i}: {str(e)}")
    
    logger.info(f"\nüìä –ò–¢–û–ì–û: {success_count} –∏–∑ {len(test_cases)} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
    return success_count == len(test_cases)

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    logger.info("üöÄ –ó–ê–ü–£–°–ö –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –í–°–ï–• –ü–†–û–ú–ü–¢–û–í")
    
    # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å
    model_name = get_fast_model()
    if not model_name:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        return
    
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    from ai_model.prompts import (
        prompt_tagger,
        prompt_comparison,
        prompt_market_impact,
        prompt_style_news,
        prompt_update_summary
    )
    
    # –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∏ –∏—Ö —Ç–µ—Å—Ç-–∫–µ–π—Å–∞–º–∏
    prompts_to_test = {
        'tagger': (prompt_tagger.SYSTEM_PROMPT_TAGGER, TEST_CASES['tagger']),
        'comparison': (prompt_comparison.SYSTEM_PROMPT, TEST_CASES['comparison']),
        'market_impact': (prompt_market_impact.SYSTEM_PROMPT, TEST_CASES['market_impact']),
        'style_news': (prompt_style_news.SYSTEM_PROMPT, TEST_CASES['style_news']),
        'update_summary': (prompt_update_summary.SYSTEM_PROMPT, TEST_CASES['update_summary']),
    }
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
    results = {}
    for prompt_type, (system_prompt, test_cases) in prompts_to_test.items():
        success = await test_prompt(prompt_type, system_prompt, test_cases, model_name)
        results[prompt_type] = success
    
    # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å–≤–æ–¥–∫—É
    logger.info("\n" + "="*80)
    logger.info("–ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê")
    logger.info("="*80)
    
    for prompt_type, success in results.items():
        status = "‚úÖ –£–°–ü–ï–®–ù–û" if success else "‚ùå –û–®–ò–ë–ö–ò"
        logger.info(f"{prompt_type.upper():<20} {status}")
    
    logger.info("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
    except Exception as e:
        logger.error(f"\n–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
